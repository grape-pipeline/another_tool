#!/usr/bin/env python
"""This module provides the basic infrasructure to submit Tools to a Grid
engine.
"""
import logging
import subprocess
import os
import sys
import time
from mako.template import Template
from another.tools import Tool
from another.pipelines import PipelineTool
import cPickle


SEP_RESULT = "-------------------RESULT-------------------"
DEFAULT_TEMPLATE = """#!/bin/bash
#
# Autogenerated script that runs
# the tool using the another_tool
# library
#
${header or ''}

${script}

"""


class ClusterException(Exception):
    """Default error returned on submission failure"""
    pass


class ToolWrapper(object):
    """Tool wrapper class that holds the tool and
    the args and kwargs of it. The class provides
    a single run() method that will execute the tool.

    This class is used to serialize a given tool and its arguments
    to disk, and, when deserialized, execute the tool.
    """

    def __init__(self, tool, args):
        """Initialize the wrapper with the tool and its arguments"""
        self.tool = tool
        self.args = args

    def run(self):
        """Run the tool and catch any exception raised by the tool
        execution. If an exception is raised, it is logged and then
        returned as results. This allows the :class:Feature to pick
        up any exceptions raised in remote execution.
        """
        try:
            result = self.tool.run(self.args)
            return result
        except Exception, e:
            sys.stderr.write("Error while executing job: %s\n" % str(e))
            return e


class Feature(object):
    """Job feature returned by a cluster after submitting a job.
    The feature sotres references to the remote jobid and the
    jobs stdout and stderr files as well as the executed tool
    and the tools arguments.
    In addition, the feature stores a job_template, a :class:JobTemplate
    that contains the runtime information about the job.
    """

    def __init__(self, jobid, tool=None, stdout=None, stderr=None):
        self.jobid = jobid
        self.stdout = stdout
        self.stderr = stderr
        self.tool = tool
        self.log = logging.getLogger("%s.%s" % (self.__module__,
                                                self.__class__.__name__))

    def get(self, cluster, check_interval=360):
        """Wait until the job is finished and returns the result of the job.

        Paramter
        --------
        cluster - the cluster that runs the job
        """
        self.wait(cluster, check_interval=check_interval)
        # try to load the result from stdout file
        result = self._load_results(self.stdout)
        if isinstance(result, Exception):
            raise result
        return result

    def _load_results(self, results):
        """Internal method to load the results
        from the given results file and return them
        """
        try:
            with open(results, 'r') as result_file:
                lines = None
                # get lines after separator line
                for line in result_file:
                    if lines is not None:
                        lines.append(line)
                    elif line.strip() == SEP_RESULT:
                        lines = []
                return cPickle.loads("".join(lines).decode('base64'))
        except Exception, e:
            self.log.error("Unable to load results from feature: %s", str(e))
            raise e

    def wait(self, cluster, check_interval=360):
        """Blocks until the jobs disappears from the cluster. No checks
        are made for success or failure state.

        Paramter
        -------
        cluster -- the cluster instance
        check_interval -- the interval in second in which the job state is
                          checked. Note that this might not be used by all
                          implementation. If there is a more efficient way
                          to get the job state rather than polling the cluster,
                          that should be used.
        """
        # wait for the job to disappear from the list
        cluster.wait(self.jobid, check_interval=check_interval)

    def cancel(self, cluster):
        """Cancel the job

        Parameter
        ---------
        cluster -- the cluster that runs the job
        """
        pass

    def get_status(self, cluster):
        """Check the state of the job on the given remote cluster"""
        pass


class Cluster(object):
    """The abstrat base class for cluster implementation consists of a single
    method that is able to submit a tool to a compute cluster. The
    tool is passed as a dump string, usually created by calling
    another.executor.dumps(). The cluster implementation should interpret this
    string as valid bash, create the environment appropriatly, and submit the
    job.

    The submission template must be further customizable by the user, where the
    following variables are allowed

     * header -- custom header that will be rendered into the template
     * script -- the tool_script

    The template is rendered using the mako library. The simplest use case
    is that variables expressed like ${script} are replaced by the template
    engine.

    In case the sumission failed, a ClusterException is raised.

    """

    def submit(self, tool, args=None):
        """Submit the tool by wrapping it into the template
        and sending it to the cluster. If the tool is a string, given args
        are ignored and the script string is added as is into the template.
        If the tool is an instance of Tool, the tools dump method is
        used to create the executable script.

        This method return the job id associated with the job by the
        unterlying grid engine.

        Parameter
        ---------
        tool -- string representation of a bash script that will run the
                tool or a tool instance that is dumped to create the script
        args -- tuple of *args and **kwargs that are passed to the tool dump
                in case the tool has to be converted to a script
        """
        template = tool.job.template
        if template is None:
            template = DEFAULT_TEMPLATE
        deps = None
        tool_script = tool
        if isinstance(tool, Tool):
            # if a tool list passed, make it
            # an executable script
            tool_script = self._dump_tool(tool, args)
            deps = tool.job.dependencies
        elif isinstance(tool, PipelineTool):
            tool_script = self._dump_tool(tool._tool, args)
            # update dependencies
            deps = [str(d.job.jobid)
                    for d in filter(lambda t: t.job.jobid is not None,
                                    tool.get_dependencies())]
        if len(deps) == 0:
            deps = None

        rendered_template = Template(template).render(script=tool_script,
                                                      max_time=tool.job.max_time,
                                                      max_mem=tool.job.max_mem,
                                                      threads=tool.job.threads,
                                                      tasks=tool.job.tasks,
                                                      queue=tool.job.queue,
                                                      header=tool.job.header,
                                                      priority=tool.job.priority)

        feature = self._submit(rendered_template,
                               name=tool.job.name,
                               max_time=tool.job.max_time,
                               max_mem=tool.job.max_mem,
                               threads=tool.job.threads,
                               tasks=tool.job.tasks,
                               queue=tool.job.queue,
                               priority=tool.job.priority,
                               dependencies=deps,
                               working_dir=tool.job.working_dir,
                               extra=tool.job.extra)
        if isinstance(tool, (Tool, PipelineTool)):
            tool.job.jobid = feature.jobid

        return feature

    def wait(self, jobid, check_interval=360):
        """Block until the job is no longer in any of the cluster queues.

        Paramter
        --------
        jobid -- the job id
        check_interval -- interval in second in which the job state should be
                          checked. If this is used depends on the cluster and
                          the implementations. If there is a better way than
                          polling the state in regular intervals, that should
                          be used in favor of the polling strategy
        """
        raise ClusterException("Wait is not implemented!")

    def dump(self, tool, args):
        """Save the given tool instance and the arguments and returns a string
        that is a valid bash script that will load and execute the tool.

        Note that the script does not set up the python environment or
        the paths. This has to be done by the script caller!

        Parameter
        ---------

        tool -- the tool that will be prepared for execution
        *args -- tool arguments
        **kwargs -- tool keyword arguments
        Returns
        -------

        script -- a string that is a valid bash script and will load and
                run the tool
        """
        template = """
python -c '
import sys;
import cPickle;
source="".join([l for l in sys.stdin]).decode("base64");
result = cPickle.loads(source).run();
# pickel the result and print it to stdout
result_string = cPickle.dumps(result).encode("base64");
print "%s"
print result_string
'<< __EOF__
%s__EOF__

"""
        wrapper = ToolWrapper(tool, args)
        return template % (SEP_RESULT, cPickle.dumps(wrapper).encode("base64"))

    def _dump_tool(self, tool, args):
        """Dump a tool instance to an executable script
        using the args and kwargs in the args paramters)
        """
        #dump the tool with arguments
        return self.dump(tool, args)

    def _submit(self, script, max_time=0, name=None,
                max_mem=0, threads=1, queue=None, priority=None, tasks=1,
                dependencies=None, working_dir=None, extra=None):
        """This method must be implemented by the subclass and
        submit the given script to the cluster. Please note that
        the script is passed as a string. It depends on the implementation
        and the grid engine if this script is supposed to be written to disk
        or can be submitted by other means.

        Parameter
        ---------
        tool_script -- the fully rendered script string
        max_time -- the maximum wallclock time of the job in seconds
        name     -- the name of the job
        max_mem  -- the maximum memory that can be allocated by the job
        threads  -- the number of cpus slots per task that should be allocated
        tasks    -- the number of tasks executed by the job
        queue    -- the queue the ob should be submitted to
        prority  -- the jobs priority
        dependencies -- list or string of job ids that this job depends on
        extra    -- list of any extra parameters that should be considered
        """
        pass

    def _add_parameter(self, params, name=None, value=None, exclude_if=None,
                       to_list=None, prefix=None):
        """This is a helper function to create paramter arrays that are passed
        to subprocess.Popen.
        If name is not none the pair (name, value) is added
        to the params list, otherwise just value is appended as long as
        the value is not a list or a tuple. Otherwise the params array is
        extended by the list/tuple.
        If valu is None noting is added to the params list. In addition
        you can specify the exclude_if fundtion. If the function is specified,
        the value is only added if the function returns False.
        Lists of values will be joined to a single string value if
        to_list is specified. The value of to_list is used to join the list
        elements.

        Paramter
        --------
        params -- the target list
        name   -- name of the paramter
        value  -- the raw value
        exclude_if -- function that should return True if the value shoudl be
                      excluded
        to_list -- if specified and the value is a list, the list is joined to
                   a string using the to_list value
        prefix  -- prefix the value before ading it to the paramter list
        """
        if value is None:
            return
        if exclude_if is not None and exclude_if(value):
            return

        # eventually convert a list value to a string
        value = self.__check_list(value, to_list)

        if prefix is not None:
            value = "%s%s" % (prefix, str(value))

        if name is not None:
            params.extend([str(name), str(value)])
        else:
            if isinstance(value, (list, tuple,)):
                params.extend(value)
            else:
                params.append(str(value))

    def __check_list(self, value, to_list):
        if to_list is not None:
            if not isinstance(value, (list, tuple,)):
                value = [value]
            value = to_list.join(value)
        return value

    def log(self):
        """Get the tool logger"""
        return logging.getLogger("%s.%s" % (self.__module__,
                                            self.__class__.__name__))


class Slurm(Cluster):
    """Slurm extension of the Cluster implementationcPickle.load(""

    The slurm implementation sends jobs to the cluster using
    the `sbatch` command line tool. The job parameter are paseed
    to `sbatch` as they are. Note that:

    * max_mem is passed as --mem-per-cpu

    """

    def __init__(self, sbatch="sbatch", squeue="squeue"):
        """Initialize the slurm cluster.

        Paramter
        --------
        sbatch -- path to the sbatch command. Defaults to 'sbatch'
        squeue -- path to the squeue command. Defaults to 'squeue'
        """
        self.sbatch = sbatch
        self.squeue = squeue

    def _submit(self, script, max_time=None, name=None,
                max_mem=None, threads=1, queue=None, priority=None, tasks=1,
                dependencies=None, working_dir=None, extra=None):
        params = [self.sbatch]

        self._add_parameter(params, "-t", max_time,
                            lambda x: x is None or int(x) <= 0)
        self._add_parameter(params, "-p", queue)
        self._add_parameter(params, "--qos", priority)
        self._add_parameter(params, "-c", threads,
                            lambda x: x is None or int(x) <= 0)
        self._add_parameter(params, "--mem-per-cpu", max_mem,
                            lambda x: x is None or int(x) <= 0)
        self._add_parameter(params, "-D", working_dir)
        self._add_parameter(params, "-d", dependencies, prefix="afterok:",
                            to_list=":")
        self._add_parameter(params, "-d", dependencies, prefix="afterok:",
                            to_list=":")
        self._add_parameter(params, "-J", name)
        self._add_parameter(params, "-e", "slurm-%j.err")
        self._add_parameter(params, "-o", "slurm-%j.out")
        self._add_parameter(params, value=extra)

        process = subprocess.Popen(params,
                                   stdin=subprocess.PIPE,
                                   stdout=subprocess.PIPE,
                                   stderr=subprocess.PIPE,
                                   shell=False)

        process.stdin.write(script)
        process.stdin.close()
        out = "".join([l for l in process.stdout])
        err = "".join([l for l in process.stderr])
        if process.wait() != 0:
            raise ClusterException("Error while submitting job:\n%s" % (err))
        job_id = out.strip().split(" ")[3]

        stdout_file = os.path.abspath("slurm-%s.out" % (job_id))
        stderr_file = os.path.abspath("slurm-%s.err" % (job_id))

        feature = Feature(jobid=job_id, stdout=stdout_file,
                          stderr=stderr_file)

        return feature

    def wait(self, jobid, check_interval=360):
        if jobid is None:
            raise ClusterException("No job id specified! Unable to check"
                                   "  job state!")

        while True:
            process = subprocess.Popen([self.squeue, '-h', '-j', str(jobid)],
                                       stderr=subprocess.PIPE,
                                       stdout=subprocess.PIPE)
            (out, err) = process.communicate()
            if process.wait() != 0 or len(out.strip()) == 0:
                return
            else:
                time.sleep(check_interval)
